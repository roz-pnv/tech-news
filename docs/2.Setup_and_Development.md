# TechNews Project Documentation

----
# 2. Setup & Development 

This document explains how to install, configure, run, and test the TechNews Django application in both local and Docker-based environments. It also covers environment variables, database setup, background services, and automated task monitoring.

##  Requirements

To run TechNews smoothly, install the following tools:

- **Python**: Version 3.12  
- **Docker & Docker Compose**: For containerized deployment  
- **Redis**: Message broker for Celery  
- **Chromium & Chromedriver**: Installed inside the container, required for scraping with Selenium  
- **Database**: SQLite is used by default and stored at `data/db.sqlite3`  
- **Monitoring**: Flower dashboard (available at `http://localhost:5555`) for inspecting background tasks  

##  Installation

You can install and run TechNews using either a manual (local) approach or Docker containers.

###  Manual Installation (Local)

Create and activate a virtual environment:

```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

Install required dependencies:

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

Launch the development server:

```bash
python manage.py runserver
```

###  Docker-Based Setup

Build and start the entire stack:

```bash
docker-compose up --build
```

This starts the following services:

- Django server (`web`) at `http://localhost:8000`  
- Redis as the message broker  
- Celery Worker for background task execution  
- Celery Beat for periodic scheduling  
- Flower dashboard at `http://localhost:5555`  

To stop services:

```bash
docker-compose down
```

##  Environment Configuration

TechNews uses a `.env` file for environment-specific variables. It’s read using `django-environ` in `settings/base.py`.

Example `.env` file:

```env
SECRET_KEY=your_secret_key
PROD=False
DJANGO_SETTINGS_MODULE=core.settings.prod
```

These variables control things like:

- Django’s secret key  
- Runtime mode (development vs. production)  
- Database and broker URLs (Redis, Celery)  

Additional variables passed via `docker-compose.yml` include:

```yaml
CELERY_BROKER=redis://redis:6379/0
REDIS_URL=redis://redis:6379
DJANGO_ALLOWED_HOSTS=localhost 127.0.0.1 0.0.0.0 [::1] rozhinpanav.pythonanywhere.com
```

##  Database Setup

TechNews uses SQLite by default. Here's how to initialize it:

Run migrations:

```bash
python manage.py makemigrations
python manage.py migrate
```

Create a superuser:

```bash
python manage.py createsuperuser
```

The SQLite database will be created at `data/db.sqlite3`.

##  Running Tests & Coverage

TechNews uses `unittest` with `coverage` for measuring test execution. To ensure reliability and maintainability, all critical features are covered through automated tests.

To execute tests and generate a coverage report:

```bash
coverage run manage.py test
coverage report
coverage html
```

This generates an HTML report in the `htmlcov` directory.  
Open `htmlcov/index.html` in your browser to view detailed coverage statistics.

## Running Background Services with Docker & Django Admin

To manage asynchronous tasks like scraping and scheduled jobs, TechNews uses **Celery** and **Celery Beat**, orchestrated entirely via Docker.

All required services—Django, Redis, Celery Worker, Celery Beat, and Flower—are automatically started using:

```bash
docker-compose up --build
```

Once the stack is running, you can create periodic tasks directly in the **Django Admin Panel**:

### Steps to Create a Scheduled Task via Admin:

1. Visit the Django admin interface at:  
   http://localhost:8000/admin

2. Login using the superuser credentials you created earlier.

3. Locate the section called `Periodic Tasks` under the **django_celery_beat** group.

4. Create or modify tasks:
   - Add **Interval Schedule** (e.g., every 10 minutes)
   - Add **Periodic Task** and link it to your task function (e.g., `core.tasks.scrape_latest_news`)
   - Provide a name, choose the interval, and enable the task.

5. Once saved, the task will be picked up automatically by the Celery worker.

### Monitor Scheduled Tasks

To monitor and inspect background task execution, access the Flower dashboard:

```bash
http://localhost:5555
```

Flower provides real-time visibility into:
- Task status (success, failure, retries)
- Worker health
- Runtime metrics

###  Note

There is no need to manually run `celery` commands locally. All services are containerized and managed via `docker-compose`. This ensures reproducibility and isolates the environment from your host system.

 ⚠️ These background services are critical for handling automation reliably and reproducibly across environments.

## Manual Scraping with Scrapy

TechNews includes a dedicated Scrapy spider called `zoomit` for scraping content manually when needed.

To run the spider directly from the terminal, use:

```bash
scrapy crawl zoomit -a pages=1,2
```

#### Arguments

- `pages`: Specifies which page numbers to scrape. You can provide a comma-separated list or a single value.
  - Example: `pages=1,2` will scrape page 1 and page 2.
  - Example: `pages=5` will scrape only page 5.

#### Output

Scraped results will be processed or stored based on your spider’s logic—either saved to a database, output to JSON, or handled by a pipeline.

> ⚠️ This command bypasses Celery scheduling and executes the scraping task immediately. It’s useful for debugging, quick scraping, or manually triggering jobs during development.

